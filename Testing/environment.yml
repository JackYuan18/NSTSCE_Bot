name: rag-testing
channels:
  - nvidia/label/cuda-12.1.0  # For CUDA support (needed for vLLM and GPU operations)
  - conda-forge
  - pytorch
  - defaults
dependencies:
  - python=3.10  # Python 3.10+ required for vLLM 0.2.6+ (uses | union type syntax)
  - pip
  - numpy>=1.19.0
  - pip:
      # ============================================
      # Core web interface (for Testing framework)
      # ============================================
      - flask>=2.0.0
      
      # ============================================
      # Dataset loading and processing
      # ============================================
      - datasets>=2.0.0
      - jsonlines>=3.0.0
      
      # ============================================
      # RAG System dependencies (Naive-RAG)
      # ============================================
      - PyPDF2>=2.0.0
      - torch>=1.13.0  # Minimum for naive-rag; self-rag uses 2.1.2 but >=1.13.0 works
      - transformers>=4.21.0  # Minimum for naive-rag; self-rag uses 4.36.2 but >=4.21.0 works
      - sentence-transformers>=2.2.0
      - faiss-cpu>=1.7.0  # Use faiss-gpu if you have GPU support and want better performance
      - huggingface-hub>=0.10.0
      
      # ============================================
      # Evaluation metrics
      # ============================================
      - nltk>=3.8.0
      - rouge-score>=0.1.2  # From self-rag, useful for evaluation
      - sacrebleu>=2.4.0  # From self-rag, useful for evaluation
      
      # ============================================
      # Self-RAG core dependencies (REQUIRED for self-rag)
      # ============================================
      - vllm>=0.2.6  # Required for Self-RAG (requires CUDA 12.1+)
      - accelerate>=0.25.0  # For model acceleration
      - bitsandbytes>=0.41.3  # For quantization support
      - xformers>=0.0.23  # For efficient attention mechanisms
      - triton>=2.1.0  # For GPU kernel compilation
      
      # ============================================
      # Self-RAG utilities and dependencies
      # ============================================
      - spacy>=3.7.2  # For NLP processing in self-rag
      - aiohttp>=3.9.1  # For async HTTP operations
      - fastapi>=0.105.0  # For API services (used by self-rag)
      - uvicorn>=0.25.0  # ASGI server for FastAPI
      - ray>=2.9.0  # For distributed computing (optional but recommended)
      - pandas>=2.0.3  # Data processing
      - pyyaml>=6.0.1  # Configuration file parsing
      - protobuf>=4.25.1  # Protocol buffers
      - safetensors>=0.4.1  # Safe tensor storage format
      - tokenizers>=0.15.0  # Fast tokenization library
      - sentencepiece>=0.1.99  # SentencePiece tokenizer
      - tiktoken>=0.5.2  # OpenAI tokenizer
      - peft>=0.7.1  # Parameter-efficient fine-tuning
      
      # ============================================
      # Optional Self-RAG dependencies
      # ============================================
      - deepspeed>=0.12.6  # For distributed training (optional, large package)
      
      # ============================================
      # ChatGPT5/OpenAI integration
      # ============================================
      - openai>=1.0.0  # For ChatGPT5 generator (self-rag uses 0.28.1, but >=1.0.0 is recommended)
      
      # ============================================
      # Common utilities (merged from both environments)
      # ============================================
      - requests>=2.25.0
      - tqdm>=4.60.0
      - packaging>=23.2
      - filelock>=3.13.1
      - psutil>=5.9.7
      - python-dateutil>=2.8.2
      - pytz>=2023.3
      - typing-extensions>=4.9.0
      - einops>=0.7.0  # Tensor operations
      - evaluate>=0.4.1  # Evaluation metrics library
      - joblib>=1.3.2  # Parallel processing utilities
      - networkx>=3.1  # Graph operations
      - tabulate>=0.9.0  # Table formatting
      
      # ============================================
      # Notes:
      # ============================================
      # 1. CUDA packages (nvidia-*) are typically installed automatically when using
      #    CUDA-enabled PyTorch. If you encounter issues with vLLM, you may need to
      #    install specific CUDA 12.1 packages manually.
      #
      # 2. For Self-RAG to work, you need:
      #    - CUDA 12.1+ installed on your system
      #    - vLLM properly configured
      #    - Sufficient GPU memory (recommended: 24GB+)
      #
      # 3. If you only need Naive-RAG, you can skip vLLM and related CUDA packages
      #    by commenting them out or using a separate environment.
      #
      # 4. To install specific CUDA 12.1 packages if needed, uncomment:
      #    - nvidia-cublas-cu12==12.1.3.1
      #    - nvidia-cuda-runtime-cu12==12.1.105
      #    - nvidia-cudnn-cu12==8.9.2.26
      #    - nvidia-cufft-cu12==11.0.2.54
      #    - nvidia-curand-cu12==10.3.2.106
      #    - nvidia-cusolver-cu12==11.4.5.107
      #    - nvidia-cusparse-cu12==12.1.0.106
      #    - nvidia-nccl-cu12==2.18.1
      #    - nvidia-nvjitlink-cu12==12.3.101
      #    - nvidia-nvtx-cu12==12.1.105
