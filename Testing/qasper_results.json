[
  {
    "article_id": "1912.01214",
    "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation",
    "question": "which multilingual approaches do they compare with?",
    "generated_answer": "the statistics of Europarl and MultiUN corpora are summarized in Table TABREF18 . we remove the multi-parallel sentences between different training corpora to ensure zero-shot settings . we use the devtest2006 as the validation set and the test2006 as the test set for Fr$rightarrow $Es and De$rightarrow $Fr .\n\n*Generated using Local T5-Small*",
    "reference_answers": [
      "BIBREF19",
      "BIBREF20",
      "multilingual NMT (MNMT) BIBREF19"
    ],
    "retrieved_context": [],
    "generator": "t5-small",
    "rag_system": "naive-rag",
    "dataset": "qasper",
    "total_questions_in_dataset": 1005,
    "total_questions_available": 3,
    "bleu_score": null,
    "exact_match": 0.0,
    "f1_score": null,
    "recall_score": 0.0,
    "rouge_l_score": null
  },
  {
    "article_id": "1912.01214",
    "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation",
    "question": "what are the pivot-based baselines?",
    "generated_answer": ":: Main Results ::: Results on Europarl Dataset. What are the pivot-based baselines? Context: Experiments::: Main Results ::: Results on Europarl Dataset. What are the pivot-based baselines? Context: Experiments:::: Main Results ::: Results on Europarl Dataset. What are the pivot-based baselines?\n\n*Generated using Local T5-Small*",
    "reference_answers": [
      "pivoting",
      "pivoting$_{\\rm m}$",
      "firstly translates a source language into the pivot language which is later translated to the target language"
    ],
    "retrieved_context": [],
    "generator": "t5-small",
    "rag_system": "naive-rag",
    "dataset": "qasper",
    "total_questions_in_dataset": 1005,
    "total_questions_available": 3,
    "bleu_score": 0.004863083928997274,
    "exact_match": 0.0,
    "f1_score": 0.06060606060606061,
    "recall_score": 0.07142857142857142,
    "rouge_l_score": 0.07023603914795624
  },
  {
    "article_id": "1912.01214",
    "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation",
    "question": "which datasets did they experiment with?",
    "generated_answer": ":: Main Results ::: Results on Europarl Dataset. We evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De) where English acts as the pivot language, its left side is the source language, and its right side is the target language . We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings\n\n*Generated using Local T5-Small*",
    "reference_answers": [
      "Europarl",
      "MultiUN",
      "Europarl BIBREF31",
      "MultiUN BIBREF32"
    ],
    "retrieved_context": [],
    "generator": "t5-small",
    "rag_system": "naive-rag",
    "dataset": "qasper",
    "total_questions_in_dataset": 1005,
    "total_questions_available": 3,
    "bleu_score": 0.003148013662646501,
    "exact_match": 0.0,
    "f1_score": 0.04166666666666667,
    "recall_score": 1.0,
    "rouge_l_score": 0.041049798115746966
  }
]